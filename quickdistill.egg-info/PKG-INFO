Metadata-Version: 2.4
Name: quickdistill
Version: 0.1.0
Summary: Fast and easy toolkit for distilling AI models
Author-email: Brett Young <bdytx5@umsystem.edu>
License: MIT
Project-URL: Homepage, https://github.com/byyoung3/quickdistill
Project-URL: Documentation, https://github.com/byyoung3/quickdistill#readme
Project-URL: Repository, https://github.com/byyoung3/quickdistill
Project-URL: Issues, https://github.com/byyoung3/quickdistill/issues
Keywords: ai,ml,distillation,evaluation,weave
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: flask>=2.0.0
Requires-Dist: flask-cors>=3.0.0
Requires-Dist: openai>=1.0.0
Requires-Dist: weave>=0.50.0
Requires-Dist: llmasajudge>=0.1.0
Requires-Dist: datasets>=2.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: isort>=5.0.0; extra == "dev"
Requires-Dist: flake8>=4.0.0; extra == "dev"

# QuickDistill

**Fast and easy toolkit for distilling AI models**

QuickDistill provides an intuitive web UI for the complete model distillation workflow:
- ğŸ“Š View and filter Weave traces from your LLM calls
- ğŸ¯ Export strong model outputs as test sets
- ğŸ”¬ Run weak models on strong outputs
- âš–ï¸ Evaluate similarity using LLM judges
- ğŸ“¥ Download evaluation datasets for analysis

## Installation

```bash
pip install quickdistill
```

## Quick Start

Launch the UI in your current directory:

```bash
quickdistill launch
```

This will:
- Start the QuickDistill server on `http://localhost:5001`
- Open the Trace Viewer in your browser
- Create local directories for projects, exports, and results

## Requirements

Set these environment variables:

```bash
export WANDB_API_KEY="your_wandb_key"          # Required for W&B Inference
export OPENROUTER_API_KEY="your_openrouter_key"  # Optional for OpenRouter models
```

Get your keys:
- W&B: https://wandb.ai/settings
- OpenRouter: https://openrouter.ai/keys

## Usage

### 1. Fetch Weave Traces

Enter your Weave project name (e.g., `username/project-name`) and click "Fetch New Project" to load traces from W&B.

### 2. Export Strong Model Outputs

- Filter traces by model or operation
- Select traces to use as ground truth
- Export to create a test set

### 3. Run Weak Models

- Select a strong model export
- Choose W&B models from the list or enter custom OpenRouter models
- Run inference to generate weak model outputs

### 4. Create Judges

Navigate to `/judge` to create LLM judges:
- **Scalar judges**: Rate similarity on a numeric scale (1-5)
- **Boolean judges**: Determine if outputs are correct/incorrect

### 5. Run Evaluations

- Select weak model results
- Choose a judge
- Run evaluation and view results in Weave

### 6. Download Datasets

Click "Download" next to any weak model result to get a clean JSON dataset with:
```json
[
  {
    "input": "question text...",
    "strong_model": "model-name",
    "strong_output": "strong response...",
    "weak_model": "model-name",
    "weak_output": "weak response..."
  }
]
```

## CLI Options

```bash
quickdistill launch                    # Launch on default port 5001
quickdistill launch --port 8080        # Launch on custom port
quickdistill launch --no-browser       # Don't open browser automatically
quickdistill launch --debug            # Run in debug mode
```

## Project Structure

When you run `quickdistill launch`, it creates these directories in your current working directory:

```
your-project/
â”œâ”€â”€ projects/                  # Cached Weave traces by project
â”‚   â””â”€â”€ username_project/
â”‚       â””â”€â”€ traces_data.json
â”œâ”€â”€ strong_exports/            # Exported strong model test sets
â”‚   â””â”€â”€ model-name_10traces.json
â”œâ”€â”€ weak_model_*.json          # Weak model inference results
â”œâ”€â”€ judges.json                # Saved judge configurations
â””â”€â”€ evaluations/               # Evaluation results
```

## Features

- **Multi-provider support**: Works with W&B Inference and OpenRouter
- **Flexible judging**: Create custom LLM judges or use pre-built ones
- **Trace filtering**: Filter by model, operation, or custom criteria
- **Batch operations**: Run multiple models and evaluations in parallel
- **Export formats**: Download clean datasets for further analysis
- **Project isolation**: Each Weave project is cached separately

## Development

Install in development mode:

```bash
git clone https://github.com/byyoung3/quickdistill.git
cd quickdistill
pip install -e .
```

## License

MIT

## Author

Brett Young (bdytx5@umsystem.edu)
